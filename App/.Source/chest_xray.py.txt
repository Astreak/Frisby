{"cells":[{"metadata":{"_uuid":"fc0fc361-b8f5-46d2-b1de-8fa5c7e74e25","_cell_guid":"23f6280e-34e6-4e57-8985-04c394d7158c","trusted":true},"cell_type":"code","source":"# %% [code]\nimport numpy as np\nimport os\nimport sys\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import *\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import *\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nplt.style.use(\"fivethirtyeight\")\n\n# %% [code]\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)\n\n# %% [code]\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path()\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [180, 180]\nEPOCHS = 25\n\n# %% [code]\nfilenames=tf.io.gfile.glob(str(GCS_PATH+\"/chest_xray/train/*/*\"))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH+\"/chest_xray/val/*/*\")))\ntrain,val=train_test_split(filenames,test_size=0.2)\n\n\n# %% [code]\nlen(train)\n\n# %% [code]\nNorm=len([i for i in train if \"NORMAL\" in i])\nPsue=len([j for j in train if \"PNEUMONIA\" in j])\nprint(f\"Normal {Norm}\")\nprint(f\"Pneumonia {Psue}\")\n\n\n# %% [code]\nTrain=tf.data.Dataset.from_tensor_slices(train)\nVal=tf.data.Dataset.from_tensor_slices(val)\nTrain\n\n# %% [code]\ndef get_label(path):\n    parts=tf.strings.split(path,os.path.sep)\n    print(parts)\n    return parts[-2]==\"PNEUMONIA\"\n\ndef decode(img):\n    img=tf.image.decode_jpeg(img,channels=3)\n    img=tf.image.convert_image_dtype(img,tf.float32)\n    return tf.image.resize(img,[180,180])\ndef process_inp(path):\n    label=get_label(path)\n    img=tf.io.read_file(path)\n    img=decode(img)\n    return img,label\n\n\n# %% [code]\nTrain=Train.map(process_inp,num_parallel_calls=AUTOTUNE)\nVal=Val.map(process_inp,num_parallel_calls=AUTOTUNE)\n\n\n# %% [code]\nfor i,j in Train.take(1):\n    print(j)\n    \n\n# %% [code]\ntest=tf.data.Dataset.list_files(str(GCS_PATH+\"/chest_xray/test/*/*\"))\ntest=test.map(process_inp,num_parallel_calls=AUTOTUNE)\n\n\n# %% [code]\ntf.data.experimental.cardinality(test).numpy()\n\n# %% [code]\nTrain=Train.batch(64,drop_remainder=True)\nVal=Val.batch(64)\nTrain=Train.prefetch(AUTOTUNE)\nVal=Val.prefetch(AUTOTUNE)\n\n\n# %% [code]\nTrain\n\n# %% [code]\nI,L=next(iter(Train))\n\n\n# %% [code]\nI=I.numpy()\nL=L.numpy()\n\n\n# %% [code]\n plt.figure(figsize=(12,12))\nfor i in range(5,20):\n   \n    ax=plt.subplot(3,5,i-5+1)\n    plt.imshow(I[i],cmap=plt.cm.jet)\n    if L[i]==False:\n        plt.title(\"NORMAL\")\n    else:\n        plt.title(\"PNEUMINIA\")\n    plt.axis(\"off\")\n    plt.savefig(\"box23.png\")\n    \n\n# %% [code]\nnormal_weight=(1/Norm)*((4185)/2.0)\npseu_weight=(1/Psue)*((4185)/2.0)\n\n\n# %% [code]\nW={0:normal_weight,1:pseu_weight}\n\n\n# %% [code]\nW\n\n# %% [code]\ndef conv_block():\n    I=Input((180,180,3))\n    C1=Conv2D(16,3,activation=\"relu\",padding=\"same\")(I)\n    M1=MaxPooling2D()(C1)\n    C2=Conv2D(16,3,activation=\"relu\",padding=\"same\")(M1)\n    M2=MaxPooling2D()(C2)\n    C3=SeparableConv2D(32,3,activation=\"relu\",padding=\"same\")(M2)\n    C4=SeparableConv2D(32,3,activation=\"relu\",padding=\"same\")(C3)\n    B1=BatchNormalization()(C4)\n    M3=MaxPooling2D(pool_size=(2,2))(B1)\n    C5=SeparableConv2D(64,3,activation=\"relu\",padding=\"same\")(M3)\n    C6=SeparableConv2D(64,3,activation=\"relu\",padding=\"same\")(C5)\n    B2=BatchNormalization()(C6)\n    M4=MaxPooling2D(pool_size=(2,2))(B2)\n    D1=Dropout(0.3)(M4)\n    C7=SeparableConv2D(128,3,activation=\"relu\",padding=\"same\")(D1)\n    C8=SeparableConv2D(128,3,activation=\"relu\",padding=\"same\")(C7)\n    B3=BatchNormalization()(C8)\n    M5=MaxPooling2D(pool_size=(2,2))(B3)\n    D2=Dropout(0.4)(M5)\n    F=Flatten()(D2)\n    Dn1=Dense(512,activation=\"relu\")(F)\n    Dn2=Dense(128,activation=\"relu\")(Dn1)\n    D3=Dropout(0.4)(Dn2)\n    Dn3=Dense(64,activation=\"relu\")(D3)\n    Dnf=Dense(1,activation=\"sigmoid\")(Dn3)\n    model=tf.keras.models.Model(I,Dnf)\n    return model\n\n            \n    \n    \n    \n    \n    \n\n# %% [code]\nwith strategy.scope():\n    model=conv_block()\n    model.summary()\n    Metrics=[\n    \"accuracy\",\n    tf.keras.metrics.Precision(),\n    tf.keras.metrics.Recall()\n    ]\n    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=Metrics)\n    \n\n\n# %% [code]\nclear\n\n# %% [code]\nTrain_count=4185\nVal_count=1097\n\n\n# %% [code]\nhistory=model.fit(Train,epochs=15,validation_data=Val,class_weight=W)\nmodel.save(\"PN.h5\")\n\n\n\n\n\n# %% [code]\n","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}